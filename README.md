# ğŸ§© LLM Evaluation & Guardrails â€” Notebook Version

![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Jupyter](https://img.shields.io/badge/Notebook-Ready-orange)

A single Jupyter Notebook to **evaluate**, **analyze**, and **stress-test** Large Language Models (LLMs).  
This project includes **metrics**, **PII detection**, **red teaming**, and **visual analytics** â€” all in one lightweight file.

---

## ğŸš€ Overview

**LLM Evaluation & Guardrails** provides a minimal yet powerful framework to test how reliable, safe, and robust a model is.  
It helps you measure **accuracy**, **PII safety**, and **robustness** with a simple notebook interface.

### âœ¨ Features
- ğŸ“Š **Metrics:** Exact Match (EM), token-level F1, latency tracking  
- ğŸ” **PII Detection:** regex-based checks for emails, phone numbers, and IBAN-like patterns  
- ğŸ§¨ **Red Teaming:** jailbreak and adversarial prompt testing  
- âš¡ **Plug-and-Play LLM Client:** compatible with Ollama (local), HTTP APIs, or mock mode  
- ğŸ“ˆ **Visualization:** charts for accuracy and latency  
- ğŸ’¾ **Export:** save results as `.csv` and `.json` for later analysis  

---

## ğŸ§  Motivation

Modern LLMs are powerful â€” but how **safe** and **consistent** are they?  
This notebook lets you locally benchmark any LLM, detect privacy leaks, and probe robustness without complex setup.

Perfect for:
- ğŸ¤– AI / ML engineers  
- ğŸ“ Students & researchers in NLP or AI safety  
- ğŸ§ª Developers testing local or hosted LLMs  

---


## ğŸ“ Project Structure

llm-eval-guardrails/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                  # Description complÃ¨te du projet (vue sur GitHub)
â”œâ”€â”€ ğŸ“¦ requirements.txt           # Liste des dÃ©pendances Python
â”œâ”€â”€ âš™ï¸ .env.example               # Exemple de configuration d'environnement
â”œâ”€â”€ ğŸš« .gitignore                 # Fichiers Ã  ignorer par Git
â”‚
â”œâ”€â”€ ğŸ““ llm_eval_guardrails.ipynb  # Notebook principal (le cÅ“ur du projet)
â”‚
â”œâ”€â”€ ğŸ“‚ app/                       # (optionnel) modules Python si tu sÃ©pares ton code
â”‚   â”œâ”€â”€ metrics.py                # Calcul des mÃ©triques EM, F1, etc.
â”‚   â”œâ”€â”€ pii_detector.py           # DÃ©tection PII (emails, phones, etc.)
â”‚   â”œâ”€â”€ redteam.py                # Prompts de red teaming / jailbreaks
â”‚   â””â”€â”€ llm_client.py             # Client pour Ollama / HTTP API
â”‚
â”œâ”€â”€ ğŸ“‚ data/                      # Jeux de tests et entrÃ©es de validation
â”‚   â”œâ”€â”€ sample_tests.json
â”‚   â””â”€â”€ redteam_prompts.txt
â”‚
â”œâ”€â”€ ğŸ“‚ artifacts/                 # Sorties gÃ©nÃ©rÃ©es
â”‚   â”œâ”€â”€ results.csv
â”‚   â””â”€â”€ results.json
â”‚
â”œâ”€â”€ ğŸ“‚ assets/                    # Images, graphiques, captures dâ€™Ã©cran
â”‚   â”œâ”€â”€ dashboard.png
â”‚   â””â”€â”€ latency_chart.png
â”‚
â””â”€â”€ ğŸ“œ LICENSE                    # Licence (MIT, Apache 2.0, etc.)

---
