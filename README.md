# llm_eval_guardrails
Ce projet permet d’évaluer la qualité, la robustesse et la sécurité des modèles de langage (LLM) à travers des tests automatisés, des métriques de performance et des vérifications de confidentialité.
