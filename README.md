# ğŸ§© LLM Evaluation & Guardrails â€” Notebook Version

![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Jupyter](https://img.shields.io/badge/Notebook-Ready-orange)

A single Jupyter Notebook to **evaluate**, **analyze**, and **stress-test** Large Language Models (LLMs).  
This project includes **metrics**, **PII detection**, **red teaming**, and **visual analytics** â€” all in one lightweight file.

---

## ğŸš€ Overview

**LLM Evaluation & Guardrails** provides a minimal yet powerful framework to test how reliable, safe, and robust a model is.  
It helps you measure **accuracy**, **PII safety**, and **robustness** with a simple notebook interface.

### âœ¨ Features
- ğŸ“Š **Metrics:** Exact Match (EM), token-level F1, latency tracking  
- ğŸ” **PII Detection:** regex-based checks for emails, phone numbers, and IBAN-like patterns  
- ğŸ§¨ **Red Teaming:** jailbreak and adversarial prompt testing  
- âš¡ **Plug-and-Play LLM Client:** compatible with Ollama (local), HTTP APIs, or mock mode  
- ğŸ“ˆ **Visualization:** charts for accuracy and latency  
- ğŸ’¾ **Export:** save results as `.csv` and `.json` for later analysis  

---

## ğŸ§  Motivation

Modern LLMs are powerful â€” but how **safe** and **consistent** are they?  
This notebook lets you locally benchmark any LLM, detect privacy leaks, and probe robustness without complex setup.

Perfect for:
- ğŸ¤– AI / ML engineers  
- ğŸ“ Students & researchers in NLP or AI safety  
- ğŸ§ª Developers testing local or hosted LLMs  

---


## ğŸ“ Project Structure

```bash
llm-eval-guardrails/
â”œâ”€â”€ llm_eval_guardrails.ipynb   # Notebook principal : Ã©valuation et guardrails LLM
â”œâ”€â”€ requirements.txt            # DÃ©pendances Python nÃ©cessaires
â”œâ”€â”€ README.md                   # Documentation du projet
â”œâ”€â”€ LICENSE                     # Licence open source (MIT)
â”œâ”€â”€ .env.example                # Exemple de configuration dâ€™environnement
â”‚
â”œâ”€â”€ app/                        # (Optionnel) scripts Python modulaires
â”‚   â”œâ”€â”€ metrics.py              # Calcul des mÃ©triques EM, F1, latence
â”‚   â”œâ”€â”€ pii_detector.py         # DÃ©tection dâ€™emails, numÃ©ros et IBAN
â”‚   â”œâ”€â”€ redteam.py              # Prompts de red teaming / jailbreak
â”‚   â””â”€â”€ llm_client.py           # Client Ollama / HTTP API / mock
â”‚
â”œâ”€â”€ data/                       # Jeux de tests (Q&A, jailbreak, etc.)
â”‚   â”œâ”€â”€ sample_tests.json       # Exemples de scÃ©narios
â”‚   â””â”€â”€ redteam_prompts.txt     # Prompts dâ€™attaque adversariale
â”‚
â”œâ”€â”€ artifacts/                  # Sorties et rapports gÃ©nÃ©rÃ©s
â”‚   â”œâ”€â”€ results.csv             # Tableau de rÃ©sultats mÃ©triques
â”‚   â””â”€â”€ results.json            # RÃ©sultats exportÃ©s en JSON
â”‚
â””â”€â”€ assets/                     # Images et illustrations du README
    â”œâ”€â”€ dashboard.png           # Exemple de dashboard
    â””â”€â”€ latency_chart.png       # Distribution de latence

â””â”€â”€ assets/ # Images et illustrations du README
â”œâ”€â”€ dashboard.png # Exemple de dashboard
â””â”€â”€ latency_chart.png # Distribution de latence

---
